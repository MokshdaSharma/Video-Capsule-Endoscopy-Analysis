{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:** The aim of the challenge was to provide an opportunity for the development, testing and evaluation of AI models for automatic classification of abnormalities captured in video capsule endoscopy (VCE) video frames. It promotes the development of vendor-independent and generalized AI-based models for automatic abnormality classification pipeline with 10 class labels:\n",
    "1.\tAngioectasia\n",
    "2.\tBleeding\n",
    "3.\tErosion\n",
    "4.\tErythema\n",
    "5.\tForeign body\n",
    "6.\tLymphangiectasia\n",
    "7.\tPolyp\n",
    "8.\tUlcer\n",
    "9.\tWorms\n",
    "10.\tNormal\n",
    "\n",
    "**Evaluation**\n",
    "1.\tGoal Metric\n",
    "    •\tBalanced Accuracy\n",
    "    •\tMean AUC\n",
    "2.\tOther Metrics\n",
    "    •\tAUC-ROC\n",
    "    •\tSpecificity\n",
    "    •\tMean Specificity\n",
    "    •\tF1 Score\n",
    "    •\tMean F1 Score\n",
    "    •\tAverage Precision\n",
    "    •\tMean Average Precision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define dataset directories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = r'C:\\Users\\Mokshda Sharma\\Desktop\\My Projects\\VCE_Analysis\\Capsule_vision\\training'\n",
    "val_dir = r'C:\\Users\\Mokshda Sharma\\Desktop\\My Projects\\VCE_Analysis\\Capsule_vision\\validation'\n",
    "test_dir = r'C:\\Users\\Mokshda Sharma\\Desktop\\My Projects\\VCE_Analysis\\Capsule_vision\\testing'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading images and preprocessing them (basically resizing)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image preprocessing and augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 37607 images belonging to 10 classes.\n",
      "Found 16132 images belonging to 10 classes.\n",
      "Found 0 images belonging to 0 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load images from directories\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir, target_size=(224, 224), batch_size=32, class_mode='categorical')\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir, target_size=(224, 224), batch_size=32, class_mode='categorical')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir, target_size=(224, 224), batch_size=32, class_mode='categorical', shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define CNN Model**\n",
    "\n",
    "With the help of transfer learning, using a base model of MobileNetV2 and training a basic CNN model with that base model for better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "# base_model = MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
    "# base_model.trainable = False  # Freeze pre-trained layers\n",
    "\n",
    "# model = tf.keras.Sequential([\n",
    "#     base_model,\n",
    "#     tf.keras.layers.GlobalAveragePooling2D(),\n",
    "#     tf.keras.layers.Dense(256, activation='relu'),\n",
    "#     tf.keras.layers.Dense(len(train_generator.class_indices), activation='softmax')\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1176/1176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1839s\u001b[0m 2s/step - accuracy: 0.7433 - loss: 1.3826 - val_accuracy: 0.7855 - val_loss: 1.0336\n",
      "Epoch 2/5\n",
      "\u001b[1m1176/1176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m664s\u001b[0m 565ms/step - accuracy: 0.7851 - loss: 1.0198 - val_accuracy: 0.7977 - val_loss: 0.9225\n",
      "Epoch 3/5\n",
      "\u001b[1m1176/1176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m621s\u001b[0m 528ms/step - accuracy: 0.7967 - loss: 0.9063 - val_accuracy: 0.8068 - val_loss: 0.8498\n",
      "Epoch 4/5\n",
      "\u001b[1m1176/1176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m617s\u001b[0m 525ms/step - accuracy: 0.8082 - loss: 0.8381 - val_accuracy: 0.8103 - val_loss: 0.7973\n",
      "Epoch 5/5\n",
      "\u001b[1m1176/1176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m812s\u001b[0m 691ms/step - accuracy: 0.8117 - loss: 0.7891 - val_accuracy: 0.8125 - val_loss: 0.7645\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1589dc6be20>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Load MobileNetV2 with pre-trained weights but exclude top layers\n",
    "base_model = MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
    "\n",
    "# Freeze all layers initially\n",
    "base_model.trainable = False\n",
    "\n",
    "# Define a new model\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(256, activation='relu', kernel_regularizer=l2(0.001)),  # Added L2 regularization\n",
    "    Dropout(0.4),  # Added dropout to prevent overfitting\n",
    "    Dense(len(train_generator.class_indices), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model with a small learning rate\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),  # Lower LR\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_generator, \n",
    "          validation_data=val_generator, \n",
    "          epochs=5, \n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze last 30 layers of MobileNetV2 for fine-tuning\n",
    "for layer in base_model.layers[-30:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Recompile with an even lower learning rate\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001),  # Reduce LR\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compile the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer='adam',\n",
    "#               loss='categorical_crossentropy',\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train and save the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1176/1176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m602s\u001b[0m 512ms/step - accuracy: 0.8359 - loss: 0.6937 - val_accuracy: 0.8362 - val_loss: 0.6684\n",
      "Epoch 2/10\n",
      "\u001b[1m1176/1176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m627s\u001b[0m 533ms/step - accuracy: 0.8430 - loss: 0.6627 - val_accuracy: 0.8422 - val_loss: 0.6403\n",
      "Epoch 3/10\n",
      "\u001b[1m1176/1176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1334s\u001b[0m 1s/step - accuracy: 0.8472 - loss: 0.6432 - val_accuracy: 0.8465 - val_loss: 0.6235\n",
      "Epoch 4/10\n",
      "\u001b[1m1176/1176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2287s\u001b[0m 2s/step - accuracy: 0.8496 - loss: 0.6214 - val_accuracy: 0.8520 - val_loss: 0.6101\n",
      "Epoch 5/10\n",
      "\u001b[1m1176/1176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2759s\u001b[0m 2s/step - accuracy: 0.8525 - loss: 0.6132 - val_accuracy: 0.8536 - val_loss: 0.6032\n",
      "Epoch 6/10\n",
      "\u001b[1m1176/1176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2259s\u001b[0m 2s/step - accuracy: 0.8583 - loss: 0.5893 - val_accuracy: 0.8582 - val_loss: 0.5823\n",
      "Epoch 7/10\n",
      "\u001b[1m1176/1176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1800s\u001b[0m 2s/step - accuracy: 0.8635 - loss: 0.5733 - val_accuracy: 0.8576 - val_loss: 0.5802\n",
      "Epoch 8/10\n",
      "\u001b[1m1176/1176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1880s\u001b[0m 2s/step - accuracy: 0.8687 - loss: 0.5470 - val_accuracy: 0.8653 - val_loss: 0.5631\n",
      "Epoch 9/10\n",
      "\u001b[1m1176/1176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1840s\u001b[0m 2s/step - accuracy: 0.8720 - loss: 0.5463 - val_accuracy: 0.8675 - val_loss: 0.5540\n",
      "Epoch 10/10\n",
      "\u001b[1m1176/1176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1862s\u001b[0m 2s/step - accuracy: 0.8727 - loss: 0.5367 - val_accuracy: 0.8682 - val_loss: 0.5428\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x158a2e6b250>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "# model.fit(train_generator, \n",
    "#           validation_data=val_generator, \n",
    "#           epochs=10, \n",
    "#           verbose=1)\n",
    "\n",
    "model.fit(train_generator, \n",
    "          validation_data=val_generator, \n",
    "          epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m505/505\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m436s\u001b[0m 863ms/step - accuracy: 0.8714 - loss: 0.5327\n",
      "Validation Accuracy: 0.8682\n",
      "Validation Loss: 0.5428\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(val_generator)\n",
    "print(f\"Validation Accuracy: {acc:.4f}\")\n",
    "print(f\"Validation Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using model on test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and preprocess test images\n",
    "def load_test_images(test_dir):\n",
    "    test_images = []\n",
    "    image_filenames = []\n",
    "    for img_name in os.listdir(test_dir):\n",
    "        img_path = os.path.join(test_dir, img_name)\n",
    "        img = load_img(img_path, target_size=(224, 224))  # Ensure correct size\n",
    "        img_array = img_to_array(img) / 255.0  # Normalize\n",
    "        test_images.append(img_array)\n",
    "        image_filenames.append(img_name)\n",
    "    return np.array(test_images), image_filenames\n",
    "\n",
    "# Load test images\n",
    "test_dir = r'C:\\Users\\Mokshda Sharma\\Desktop\\My Projects\\VCE_Analysis\\Capsule_vision\\testing'\n",
    "test_images, test_filenames = load_test_images(test_dir)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(test_images)\n",
    "\n",
    "# Get class labels\n",
    "class_labels = list(train_generator.class_indices.keys())\n",
    "\n",
    "# Print predictions\n",
    "for i, pred in enumerate(predictions):\n",
    "    predicted_class = class_labels[np.argmax(pred)]\n",
    "    print(f\"Image: {test_filenames[i]} -> Predicted: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saves the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"disease_model.h5\")  # Saves the entire model (architecture + weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
